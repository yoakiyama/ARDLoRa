{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA fine-tuning with automatic relevance determination\n",
    "\n",
    "Full rank matrix $V \\in \\mathbb{R}^{F \\times N}$\n",
    "\n",
    "Factorize as $V' \\stackrel{\\Delta}{\\approx} WH$\n",
    "\n",
    "Objective function:\n",
    "\n",
    "$ C(W, H, \\lambda) \\stackrel{\\Delta}{\\approx} âˆ’ log p(W, H, \\lambda | V) = \\frac{1}{\\phi} D_{\\beta}(V|WH) + \\sum_{k=1}^{K} \\frac{1}{\\lambda_k} (f(w_k) + f(h_k) + b) + c \\log \\lambda_k + \\text{cst}$\n",
    "\n",
    "Using L-1 regularization, we define\n",
    "\n",
    "$f(x) = \\| x \\|_1$ and $c = F + N + a + 1$\n",
    "\n",
    "\n",
    "```python\n",
    "loss = beta_div(Beta,V,W,H,eps_,mask)\n",
    "cst = (K*C)*(1.0-torch.log(C))\n",
    "return torch.pow(phi,-1)*loss + (C*torch.sum(torch.log(lambda_ * C))) + cst\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        init_rank: int,\n",
    "        alpha: float = 1.,\n",
    "        a: float = 1.,\n",
    "        phi: float = 1.,\n",
    "        dropout: float = 0.,\n",
    "        # merge_weights: bool = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.init_rank = init_rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / init_rank\n",
    "        self.dropout = dropout\n",
    "        # Initialize A and B matrices\n",
    "        self.lora_A = nn.Parameter(torch.empty(in_dim, init_rank))\n",
    "        self.lora_B = nn.Parameter(torch.empty(init_rank, out_dim))\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        # Initialize constants\n",
    "        self.register_buffer('c', torch.tensor(((in_dim + out_dim) / 2) + a + 1))\n",
    "        self.register_buffer('cst', torch.tensor((init_rank * self.c) * (1 - torch.log(self.c))))\n",
    "        self.register_buffer('phi', torch.tensor(phi))\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(p = dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (self.dropout(x) @ self.lora_A @ self.lora_B)\n",
    "        return x\n",
    "\n",
    "class LoRALinear(Module):\n",
    "    def __init__(self, linear_layer, init_rank, alpha=1, a=1, phi=1, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.base_layer = linear_layer\n",
    "        self.lora = LoRALayer(linear_layer.in_features, linear_layer.out_features, init_rank, alpha, a, phi)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_layer(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions \n",
    "def compute_sparsity_loss(A, B, c, b, cst):\n",
    "    lambda_k = torch.div(0.5*torch.sum(A ** 2, dim=0) + 0.5*torch.sum(B ** 2, dim=1), c)\n",
    "    return c * torch.sum(torch.log(lambda_k + b)) + cst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(model, init_rank, alpha=1):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Replace linear layers with LoRA layers\n",
    "            parent_name = '.'.join(name.split('.')[:-1])\n",
    "            layer_name = name.split('.')[-1]\n",
    "            parent = model.get_submodule(parent_name)\n",
    "            lora_layer = LoRALinear(module, init_rank, alpha)\n",
    "            setattr(parent, layer_name, lora_layer)\n",
    "    # Freeze base model parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Unfreeze LoRA parameters\n",
    "    for param in model.named_parameters():\n",
    "        if 'lora' in name:\n",
    "            param.requires_grad = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
